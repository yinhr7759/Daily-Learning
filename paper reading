Accurate Monocular Object Detection via Color-Embedded 3D Reconstruction for Autonomous Driving
不同于以往侧重于从二维图像中提取rgb特征的方法，

这篇文章提出的框架侧重在重建的三维空间里进行3d检测。

首先用一个独立的模块将输入数据从2d图像转换到3d点云，再用

pointnet获取目标的3d位置 尺寸 方向等信息。

为了增强点云的判别能力，提出一个多模态特征融合模块，将rgb嵌

入到生成的点云中去；与图像平面相比，从生成的3d场景空间中预

测3d框更有效

前面是分析了下基于单目图像的方法相较于基于lidar的成本低，但他效果不如基于lidar的好，本文提出数据表示形式的转换来提高基于单目图像的3d检测的性能

接下来分析比较经典的基于单目的3d检测方法，采用的是类似2d检测器的pipeline，主要针对2d图像中提取到的rgb特征，但这些不适用于3d任务，因为它缺少空间信息，这也是为什么早期这方面的研究没有取得好的效果的主要原因之一。

然后在分析了，可以用cnn去预测深度图用作输入，但简单的将其用作rgb图像的附加通道是不足以弥补 基于单目和基于lidar方法的差距的，所以我们需要点云这种能描述真实世界的三维坐标的表示形式而不是图像中的相对位置的深度

再罗列了把深度图转换成点云的好处，能直观显示空间信息，网络能更容易学习到输入到输出的非线性映射，某些结构仅存在3d空间，网络能学到更丰富的信息，能用来得到更好的3d检测结果

基于这些提出一种基于单目图像的3d检测框架，设计的主要思想是找到更好的输入表示形式。具体就是，首先使用前端的cnn和rgb数据去生成检测和深度估计的两个中间层；然后使用相机的校准文件将深度图转换成点云，将其作为后续的输入；rgb聚合模块保证提出方法的性能，将点云和rgb信息聚合后，进一步增强了3d目标的特征判别性。

分析了mono3d 3dop deepmanta等一些基于单目图像的方法，虽然他们都利用了一些有效的先验知识或者合理约束条件，比如目标大小，地平面，但他们缺少空间信息，再提到这篇文章虽然利用独立模块去估计视差信息并且将其和rgb信息编码到输入数据中，但没有充分利用他们，相比之下这篇文章提出的方法将深度作为核心特征，并把它转换成点云充分利用空间信息

再分析了几种基于lidar的方法，提到他们没有利用rgb信息，于是在此基础引入rgb特征融合模块增强点云的判别性

下面是介绍提出的框架，主要分为3d数据生成和3d框估计两个部分。第一个部分，先利用2个网络去实现2d检测和深度估计分别获取位置和深度信息。使用2d检测是为了获取目标所在的roi，再提取每个roi中的点作为后续的输入。进行3d框估计的时候，使用悲剧点分割和rgb信息聚合两个模块去改进整体性能，最后使用pointnet作为backbone预测每个roi的3d位置 尺寸和方向

首先是数据转换。这里是借助相机校准文件来将深度图转换成点云的方法，f是相机焦距，cx cy是相机坐标系的原点，uv是2d图像中的像素点坐标，xyz是3维点云坐标，d是深度；

这里再提到其实使用点云编解码器网络学习uv到xyz的映射也是可以的，这两种方法没有太大的差异，因为深度图本身的噪声引入的误差远大于点云生成阶段的误差

接下来是点云背景点分割，提到有人已经用实例分割的方法来实现这一目标，但是需要进行额外的预处理，而且同样会引入误差，所以这里首先按顺序计算每个2d框中的深度平均值，再将其作为阈值，z通道值大于这个阈值的都被认为是背景点，这是处理后的表示形式，pv是z通道值，其实就是深度，r是偏差用于修正阈值。生成的点集 s‘  作为后续的输入

然后是3d框估计，在这之前先使用这篇文章的方法去预测roi的中心，然后用下面这个公式去更新点集s’ ，得到s”  ，s“是我们最后3d框估计阶段的输入，使用poinetnet作为backone去预测3d框

最后再介绍rgb信息聚合模块，把公式5替换公式2，D函数输出是输入点对应的rgb值，然后我们得到一个6维向量表示每个点

但这种简单的方法去使用rgb信息是不行的，因此使用了注意力机制，首先根据特征图生成注意力图，再用其去更新点云特征图。出了点级特征融合之外，还引入一个分支提供对象级的rgb信息。先根据2d框从rgb信息中裁剪得到目标的roi，再resize；利用cnn去提取目标特征Fobj，最后是特征融合，把点级特征和目标级特征串联在一起

（先通过max pooling和average pooling得到点云空间信息的feature map，然后学习出attention map并乘上点云颜色信息的feature map，以有效指导RGB特征到坐标特征的**信息传递**）
